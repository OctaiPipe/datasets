{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "# Always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from cmapss.utils.utils import (clean_test_dataf, clean_train_dataf,\n",
    "                                encode_rul, get_rejected_features,\n",
    "                                lag_dataframe, scale_test_dataf,\n",
    "                                scale_train_dataf)\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['machine_number', 'uptime', 'setting_1', 'setting_2', 'setting_3']\n",
    "sensor_measurements = [f'sensor_{i:02d}' for i in range(1, 25)]\n",
    "columns += sensor_measurements\n",
    "\n",
    "dataset = load_dataset(\"csv\",\n",
    "                       data_files={\"train\": \"cmapss/train_FD001.csv\",\n",
    "                                   \"test\": \"cmapss/test_FD001.csv\"},\n",
    "                       sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rejected_features = get_rejected_features(filename=\"./cmapss/rejected_features_FD001.txt\")\n",
    "\n",
    "# Drop rejected features from both train and test set\n",
    "dataset = dataset.remove_columns(rejected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, we convert the dataset to a `pandas` structure. In the future,\n",
    "# the plan is to utilise the `datasets` functionality more in order to\n",
    "# standardise the processing method.\n",
    "df_train = dataset[\"train\"].to_pandas()\n",
    "df_test = dataset[\"test\"].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df_train, df_test):\n",
    "    num_lags = 30\n",
    "\n",
    "    # Run pre-processing on train set with encoded RUL\n",
    "    clean_df_train = (df_train\n",
    "                      .pipe(clean_train_dataf, rejected_features=None)\n",
    "                      .pipe(encode_rul)\n",
    "                      .drop(columns=['uptime'])\n",
    "                      )\n",
    "    # Split this pipe step to store scaler for later reuse\n",
    "    clean_df_train, scaler = clean_df_train.pipe(scale_train_dataf)\n",
    "    clean_df_train = clean_df_train.pipe(lag_dataframe, num_lags=num_lags)\n",
    "\n",
    "    # Run pre-processing on test set with encoded RUL\n",
    "    clean_df_test = (df_test\n",
    "                     .pipe(clean_test_dataf, rejected_features=None)\n",
    "                     .pipe(encode_rul)\n",
    "                     .drop(columns=['uptime'])\n",
    "                    )\n",
    "    clean_df_test = clean_df_test.pipe(scale_test_dataf, scaler=scaler)\n",
    "    clean_df_test = clean_df_test.pipe(lag_dataframe, num_lags=num_lags)\n",
    "\n",
    "    return clean_df_train, clean_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df_train, clean_df_test = preprocess_data(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = clean_df_train.copy()\n",
    "y_train = x_train.pop('RUL').astype(float)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=3)\n",
    "\n",
    "x_test = clean_df_test.copy()\n",
    "y_test = x_test.pop('RUL').astype(float)\n",
    "\n",
    "# Drop 'machine_number' from the train/test\n",
    "machine_number = x_train.pop('machine_number')\n",
    "_ = x_test.pop('machine_number')\n",
    "\n",
    "# x_train, x_val, y_train, y_val = train_test_split(\n",
    "#     x_train, y_train, test_size=0.2, random_state=42, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    # Define a custom scheduler to vary learning rate based on epoch\n",
    "    # Based on: https://doi.org/10.1016/j.ress.2017.11.021, page 5\n",
    "    if epoch < 100:\n",
    "      return lr\n",
    "    else:\n",
    "      lr = 1e-4\n",
    "      return lr\n",
    "\n",
    "# Create callback for scheduling learning rate\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model using the functional API approach\n",
    "num_features = x_train.shape[1]\n",
    "inputs = keras.Input(shape=(num_features,))\n",
    "x = layers.Dense(800, activation=\"relu\")(inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(700, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(3, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"classification_model\")\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics='accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_idx, val_idx in gss.split(x_train, y_train, groups=machine_number):\n",
    "    x_val = x_train.iloc[val_idx]\n",
    "    y_val = y_train[val_idx]\n",
    "    x_train = x_train.iloc[train_idx]\n",
    "    y_train = y_train[train_idx]\n",
    "\n",
    "    history = model.fit(x=x_train,\n",
    "                        y=y_train,\n",
    "                        epochs=5,\n",
    "                        # Suppress logging.\n",
    "                        # verbose=0,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        callbacks=[callback],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    axs[0].plot(history.history['loss'], label='train_loss')\n",
    "    axs[0].plot(history.history['val_loss'], label='val_loss')\n",
    "    axs[1].plot(history.history['accuracy'], label='train_acc')\n",
    "    axs[1].plot(history.history['val_accuracy'], label='val_acc')\n",
    "    for ax in axs:\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "    axs[0].set_ylabel('RUL')\n",
    "    axs[1].set_ylabel('Acc RUL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "y_test_ = tf.keras.utils.to_categorical(y_test, num_classes=3)\n",
    "results = model.evaluate(x_test, y_test_)\n",
    "print(\"test loss, test accuracy:\", results)\n",
    "# print(f\"test rmse {np.sqrt(results[1]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e967dfec8c9d5da3d67d336663dd03311a0fe8d3e3a24cd22fb64cee10495d21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
