{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b628bcf0",
   "metadata": {},
   "source": [
    "# CMAPSS using Deep Convolutional Neural Networks\n",
    "This notebook contains a minimal example of DCNN for the CMAPSS datasets, including\n",
    "1. Updated and refactored data preprocessing scripts\n",
    "2. CNN architecture and training steps\n",
    "\n",
    "```\n",
    "Author: CSN\n",
    "Last modified: 20221102\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c7a0a4-cbf5-49f2-b250-e9d85fcca44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "# Always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2\n",
    "\n",
    "# Import all relevant libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.utils import (clean_train_dataf, clean_test_dataf,\n",
    "                         scale_train_dataf, scale_test_dataf,\n",
    "                         lag_dataframe, shape_dataframe_to_sequence)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf7f361-44be-47cc-8493-d114962ba8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './data/train_FD001.tar.gz'\n",
    "\n",
    "# Set the column names for the raw CMAPSS data\n",
    "columns = ['machine_number', 'uptime', 'setting_1', 'setting_2', 'setting_3']\n",
    "sensor_measurements = [f'sensor_{i:02d}' for i in range(1, 25)]\n",
    "columns += sensor_measurements\n",
    "\n",
    "# Read plain text file\n",
    "df = pd.read_csv(filename, sep=\" \", names=columns, index_col=False)\n",
    "\n",
    "# Drop dummy columns\n",
    "df.drop(columns=['sensor_22', 'sensor_23', 'sensor_24'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fc9030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supplementary cell to write data for PCS-CMAPSS\n",
    "def add_train_RUL(grp):\n",
    "    # Apply clipping to RUL based on uptime\n",
    "    grp['RUL'] = (grp['uptime'].max() - grp['uptime'] + 1)\n",
    "    # Apply clipping to RUL\n",
    "    grp['RUL'].clip(upper=125, inplace=True)\n",
    "\n",
    "    return grp\n",
    "\n",
    "# Verify the number of columns in the training df\n",
    "(df\n",
    "  .sort_values(by=['machine_number', 'uptime'], axis=0)\n",
    "  .groupby('machine_number', group_keys=True)\n",
    "  .apply(add_train_RUL)\n",
    ").to_csv('./data/train_FD001.csv', index=False)\n",
    "pd.read_csv('./data/train_FD001.csv').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd5aa91-2258-407e-8537-5e004e248cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the profiler to remove constant values\n",
    "from pandas_profiling import ProfileReport\n",
    "profile = ProfileReport(df)\n",
    "# profile.to_file(\"FD001.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ecd3b2",
   "metadata": {},
   "source": [
    "Next we load or recomputed the rejected features. This list of rejected features is quite well known throughout literature. The main reasons why they are selected is because they contain constants, whereas other features contain some form of trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6526ae0-b229-497f-9d01-77ca55835c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rejected_feature_fname = './data/rejected_features.txt'\n",
    "if os.path.exists(rejected_feature_fname):\n",
    "    # Check if has feature selection has been done and load the file\n",
    "    with open(rejected_feature_fname, 'r') as f:\n",
    "        rejected_features = f.read().splitlines()\n",
    "else:\n",
    "    # Get list of rejected variables due to constant values\n",
    "    rejected_features = list(profile.get_rejected_variables())\n",
    "    # Remove sensor_06 which is also constant. Comes from viewing the data.\n",
    "    rejected_features += ['setting_1', 'setting_2', 'sensor_06']\n",
    "    # Write to file\n",
    "    with open(rejected_feature_fname, 'w') as f:\n",
    "        for feature in rejected_features:\n",
    "            f.write(f\"{feature}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152366f0",
   "metadata": {},
   "source": [
    "Now we load the test datasets for the final scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a83436-7044-4951-827c-5e16ebcbbbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now load the test set and apply the same preprocessing\n",
    "df_test = pd.read_csv('./data/test_FD001.tar.gz', sep=\" \", names=columns, index_col=False)\n",
    "\n",
    "# Drop dummy columns\n",
    "df_test.drop(columns=['sensor_22', 'sensor_23', 'sensor_24'], inplace=True)\n",
    "\n",
    "# Load the y_test\n",
    "y_test = pd.read_csv('./data/RUL_FD001.tar.gz', names=['RUL'])\n",
    "y_test['machine_number'] = y_test.index + 1\n",
    "\n",
    "# Join the test X and label dataframes\n",
    "df_test = df_test.join(y_test.set_index('machine_number'), on='machine_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316e5b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supplementary cell to write data for PCS-CMAPSS demo\n",
    "def add_test_RUL(grp):\n",
    "    grp['RUL'] += grp['uptime'].max() - grp['uptime']\n",
    "    # Apply clipping to RUL\n",
    "    grp['RUL'].clip(upper=125, inplace=True)\n",
    "    return grp\n",
    "\n",
    "# Verify the number of columsn in the test set\n",
    "(df_test\n",
    "  .sort_values(['machine_number', 'uptime'], axis=0)\n",
    "  .groupby('machine_number', group_keys=True)\n",
    "  .apply(add_test_RUL)\n",
    ").to_csv('./data/test_FD001.csv', index=False)\n",
    "pd.read_csv('./data/test_FD001.csv').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432422c8",
   "metadata": {},
   "source": [
    "In the next cell we preprocess all the data using the `pandas` pipeline functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d77dd4-b353-4ac9-b565-68ea96e61a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lags = 30\n",
    "\n",
    "# Run pre-processing on train set\n",
    "clean_df = (df\n",
    "            .pipe(clean_train_dataf, rejected_features=rejected_features)\n",
    "            .drop(columns=['uptime'])\n",
    "            )\n",
    "# Split this pipe step to store scaler for later reuse\n",
    "clean_df, scaler = clean_df.pipe(scale_train_dataf)\n",
    "# clean_df = clean_df.pipe(lag_dataframe, num_lags=num_lags)\n",
    "\n",
    "# Run pre-processing on test set\n",
    "clean_df_test = (df_test\n",
    "                 .pipe(clean_test_dataf, rejected_features=rejected_features)\n",
    "                 .drop(columns=['uptime'])\n",
    "                )\n",
    "clean_df_test = clean_df_test.pipe(scale_test_dataf, scaler=scaler)\n",
    "# clean_df_test = clean_df_test.pipe(lag_dataframe, num_lags=num_lags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb42084",
   "metadata": {},
   "source": [
    "Now we construct our train, test, and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc5a1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = shape_dataframe_to_sequence(clean_df, num_lags)\n",
    "x_test, y_test = shape_dataframe_to_sequence(clean_df_test, num_lags)\n",
    "\n",
    "# Make sure input array has shape (num_lags, num_sensors, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
    "y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "x_test = tf.convert_to_tensor(x_test, dtype=tf.float32 )\n",
    "y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "x_val = tf.convert_to_tensor(x_val, dtype=tf.float32 )\n",
    "y_val = tf.convert_to_tensor(y_val, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f991cd4",
   "metadata": {},
   "source": [
    "# Training and evaluation\n",
    "This section contains a basic MLP model and training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253a20a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    # Define an rms loss function\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    # Define a custom scheduler to vary learning rate based on epoch\n",
    "    # Based on: https://doi.org/10.1016/j.ress.2017.11.021, page 5\n",
    "    if epoch < 100:\n",
    "      return lr\n",
    "    else:\n",
    "      lr = 1e-4\n",
    "      return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3c4038-ff44-43c7-9f14-8a0408883994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model using the functional API approach\n",
    "filters, kernel_size = 10, 10  # num of convnet filters, size of filter kernel\n",
    "input_shape = np.shape(x_train)[1:]\n",
    "inputs = keras.Input(shape=input_shape)\n",
    "x = layers.Conv1D(filters, kernel_size, padding=\"same\", activation=\"tanh\")(inputs)\n",
    "x = layers.Conv1D(filters, kernel_size, padding=\"same\", activation=\"tanh\")(x)\n",
    "x = layers.Conv1D(filters, kernel_size, padding=\"same\", activation=\"tanh\")(x)\n",
    "x = layers.Conv1D(filters, kernel_size, padding=\"same\", activation=\"tanh\")(x)\n",
    "x = layers.Conv1D(1, 3, padding=\"same\", activation=\"tanh\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(32)(x)\n",
    "outputs = layers.Dense(1)(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"regress_model\")\n",
    "\n",
    "model.compile(\n",
    "    loss=root_mean_squared_error,\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=['mse', 'mae', 'mape']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91241aac-96b4-4bdf-b5f7-c83544b7ca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callback for scheduling learning rate\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "history = model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    epochs=10,\n",
    "    # Suppress logging.\n",
    "    # verbose=0,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[callback],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a59e9f1-b709-49af-a8e2-caaca45f1f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    axs[0].plot(history.history['loss'], label='train_loss')\n",
    "    axs[0].plot(history.history['val_loss'], label='val_loss')\n",
    "    axs[1].plot(history.history['mse'], label='train_mse')\n",
    "    axs[1].plot(history.history['val_mse'], label='val_mse')\n",
    "    for ax in axs:\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "    axs[0].set_ylabel(' RUL')\n",
    "    axs[1].set_ylabel('MSE RUL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f1a687-2c88-4023-9636-3fbd8b478a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7dc562",
   "metadata": {},
   "source": [
    "# Visualise prediction on the test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c455ee",
   "metadata": {},
   "source": [
    "Here, we run a prediction on the test set, which the model has not seen before. We also visualise a subset of the predictions against the ground truth as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e821fb-e679-4cb8-9499-8d99f05e53d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_ = 3500\n",
    "plt.figure(figsize=(14, 3))\n",
    "plt.plot(model.predict(x_test[:ind_, :]))\n",
    "plt.plot(y_test[:ind_], '--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac319df4",
   "metadata": {},
   "source": [
    "# Evaluate prediction on the test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524ac32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(x_test, y_test)\n",
    "print(\"test loss, test mse, test mae, test mape:\", results)\n",
    "print(f\"test rmse {np.sqrt(results[1]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a27b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netron\n",
    "model.save('./data/convnet_li_tanh.h5')\n",
    "netron.start('./data/convnet_li_tanh.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef97e40",
   "metadata": {},
   "source": [
    "## Note\n",
    "It is a good practice to re-train the model multiple times and evaluate the mean and standard deviation of the rmse. This is due to the randomness of the initialisations as well as the dropout rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a0313aecded13f319b7fc0c7b724da3001b79780c126ff78d9f7038d9403907c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
